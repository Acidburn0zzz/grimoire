           SPELL=www-robotrules
         VERSION=6.02
          SOURCE="WWW-RobotRules-${VERSION}.tar.gz"
   SOURCE_URL[0]=$PERL_CPAN_URL/authors/id/G/GA/GAAS/${SOURCE}
     SOURCE_HASH=sha512:781a2fc90e4efed901a91ecf4f2aaac2684409c6a75a8434ff140654838bb05d11f8fa760642e20eee96450d3ced6815a4dab11a4245bc8120c69ca68ab49e44
SOURCE_DIRECTORY="${BUILD_DIRECTORY}/WWW-RobotRules-${VERSION}"
        WEB_SITE="http://search.cpan.org/~LWWWP/WWW-RobotRules/"
      LICENSE[0]=ART
         ENTERED=20120902
           SHORT="database of robots.txt-derived permissions"
cat << EOF
This module parses /robots.txt files as specified in "A Standard for Robot
Exclusion", at <http://www.robotstxt.org/wc/norobots.html> Webmasters can
use the /robots.txt file to forbid conforming robots from accessing parts
of their web site.

The parsed files are kept in a WWW::RobotRules object, and this object
provides methods to check if access to a given URL is prohibited. The same
WWW::RobotRules object can be used for one or more parsed /robots.txt files
on any number of hosts.
EOF
